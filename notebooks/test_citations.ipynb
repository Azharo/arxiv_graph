{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import ray\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from pathlib import Path\n",
    "sys.path.append(os.path.abspath(\"/home/arxiv/doc_intel_etl\"))\n",
    "os.environ['PYTHONPATH'] = os.path.dirname(os.getcwd())\n",
    "import config\n",
    "import src.blob_data_transfer as blob_pull\n",
    "from regex_arxiv import REGEX_ARXIV_FLEXIBLE, clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type = 'txt'\n",
    "year_del = 3\n",
    "prefix = 'arxiv_training_data/pdfplumber/text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_FLEX = re.compile(REGEX_ARXIV_FLEXIBLE)\n",
    "RE_OLDNAME_SPLIT = re.compile(r\"([a-z\\-]+)(\\d+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGet list of text files within our blob container. They should be in the following paths:\\n\\nContianer:\\n    arxiv:\\n        arxiv_dl:\\n        arxiv_pdf:\\n        arxiv_training_data:\\n            images:\\n            pdfplumber:\\n                chars:\\n                words:\\n                text:\\n                    year:\\n                        *.txt\\nWe then only want the list of blobs within /arxiv_training_data/pdfplumber/text/*.txt\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Get list of text files within our blob container. They should be in the following paths:\n",
    "\n",
    "Contianer:\n",
    "    arxiv:\n",
    "        arxiv_dl:\n",
    "        arxiv_pdf:\n",
    "        arxiv_training_data:\n",
    "            images:\n",
    "            pdfplumber:\n",
    "                chars:\n",
    "                words:\n",
    "                text:\n",
    "                    year:\n",
    "                        *.txt\n",
    "We then only want the list of blobs within /arxiv_training_data/pdfplumber/text/*.txt\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_blob_list = blob_pull.get_blob_list(prefix)\n",
    "blob_list, year_list = blob_pull.get_blob_file_list(file_type, full_blob_list, year_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-04 02:47:52,786\tINFO resource_spec.py:212 -- Starting Ray with 37.11 GiB memory available for workers and up to 18.56 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-04 02:47:53,194\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n",
      "2020-08-04 02:47:53,198\tWARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.17.0.2',\n",
       " 'raylet_ip_address': '172.17.0.2',\n",
       " 'redis_address': '172.17.0.2:64980',\n",
       " 'object_store_address': '/tmp/ray/session_2020-08-04_02-47-52_784029_157/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-08-04_02-47-52_784029_157/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-08-04_02-47-52_784029_157'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-04 02:47:54,390\tWARNING worker.py:1090 -- The dashboard on node bc572e8fa404 failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 1220, in <module>\n",
      "    dashboard.run()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ray/dashboard/dashboard.py\", line 594, in run\n",
      "    aiohttp.web.run_app(self.app, host=self.host, port=self.port)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/aiohttp/web.py\", line 433, in run_app\n",
      "    reuse_port=reuse_port))\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 587, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/aiohttp/web.py\", line 359, in _run_app\n",
      "    await site.start()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/aiohttp/web_runner.py\", line 104, in start\n",
      "    reuse_port=self._reuse_port)\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1389, in create_server\n",
      "    % (sa, err.strerror.lower())) from None\n",
      "OSError: [Errno 99] error while attempting to bind on address ('::1', 8265, 0, 0): cannot assign requested address\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get blob list\n",
    "2. send individual blob in list to path_to_id to get the arxiv id\n",
    "3. get the text from the blob by streaming it from get_text_stream\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nHere what we're doing is creating a json line format file where\\neach line is essentially a json document. In our case each line is the\\njson document of the articles and their citations. Need to seperate each\\nline in the jsonl with the '\\n'.\\nsource: \\nhttps://medium.com/@galea/how-to-love-jsonl-using-json-line-format-in-your-workflow-b6884f65175b\\n\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def path_to_id(blob):\n",
    "    \"\"\"\n",
    "    Convert filepath name of ArXiv file to ArXiv ID.\n",
    "    Need to remove the \".txt\" from file names first if they have it\n",
    "    \"\"\"\n",
    "    name = os.path.splitext(os.path.basename(blob))[0]\n",
    "    name.replace('.txt','')\n",
    "    if '.' in name:  # new  ID\n",
    "        return name \n",
    "    split = [a for a in RE_OLDNAME_SPLIT.split(name) if a]\n",
    "    return \"/\".join(split)\n",
    "\n",
    "def get_text_stream(blob):\n",
    "        return blob_pull.stream_blob(blob).decode()\n",
    "\n",
    "def extract_references(txt, pattern=RE_FLEX):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        filename : str\n",
    "            name of file to search for pattern\n",
    "        pattern : re pattern object\n",
    "            compiled regex pattern\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        citations : list\n",
    "            list of found arXiv IDs\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for matches in pattern.findall(txt):\n",
    "        out.extend([clean(a) for a in matches if a])\n",
    "    return list(set(out))\n",
    "\n",
    "@ray.remote\n",
    "def citation_list_inner(article):\n",
    "    \"\"\" Find references in all the input articles\n",
    "    Parameters\n",
    "    ----------\n",
    "        article : str\n",
    "            path to article blob\n",
    "    Returns\n",
    "    -------\n",
    "        citations : dict[arXiv ID] = list of arXiv IDs\n",
    "            dictionary of articles and their references\n",
    "    \"\"\"\n",
    "    cites = {}\n",
    "    try:\n",
    "        article_text = get_text_stream(article)\n",
    "        refs = extract_references(article_text)\n",
    "        cites[path_to_id(article)] = refs\n",
    "        return cites\n",
    "    except Exception as e:\n",
    "        print(\"Error in {}\".format(article))\n",
    "        print(e)\n",
    "        #log.error(\"Error in {}\".format(article))\n",
    "\n",
    "def default_filename():\n",
    "    return os.path.join(os.getcwd(), 'test.json.gz')\n",
    "\n",
    "def save_to_default_location(citations):\n",
    "    filename = default_filename()\n",
    "    \n",
    "#     if not os.path.isfile(filename):\n",
    "#         with gzip.open(filename, 'w') as fn:\n",
    "#             json.dump(json.dumps(citations), fn)\n",
    "#     else:\n",
    "#         with gzip.open(filename, 'r+') as fn:\n",
    "#         # appending json data\n",
    "#             data = json.load(fn)\n",
    "#             data.update(json.dumps(citations).encode('utf-8'))\n",
    "#             fn.seek(0)\n",
    "#             json.dump(data, fn)\n",
    "#             fn.close()\n",
    "    with gzip.open(filename, 'a+') as fn:\n",
    "        json_data = json.dumps(citations).encode('utf-8')\n",
    "        fn.write(json_data + '\\n'.encode('utf-8'))\n",
    "''' \n",
    "Here what we're doing is creating a json line format file where\n",
    "each line is essentially a json document. In our case each line is the\n",
    "json document of the articles and their citations. Need to seperate each\n",
    "line in the jsonl with the '\\n'.\n",
    "source: \n",
    "https://medium.com/@galea/how-to-love-jsonl-using-json-line-format-in-your-workflow-b6884f65175b\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_path = blob_pull.copy_blob(blob_list[-100:])\n",
    "# # get all text files\n",
    "# articles = []\n",
    "# articles.extend(glob.glob(text_path+'/*.txt'))\n",
    "cites = ray.get([citation_list_inner.remote(article) for article in blob_list[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-6c3e86a1cf0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "json.loads(json.dump(cites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_default_location(cites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/arxiv/doc_intel_etl/notebooks/test.json.gz'\n",
    "cite = []\n",
    "with gzip.open(file, 'r') as f:\n",
    "    for line in f:\n",
    "        cite.extend(json.loads(line))#.rstrip('\\n').decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = json.loads(cite[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'1401.8154': ['math/0408008', '1308.1172']},\n",
       " {'1401.8156': []},\n",
       " {'1401.8181': []},\n",
       " {'1401.8182': []},\n",
       " {'1401.8201': ['1406.0349']},\n",
       " {'1401.8202': []},\n",
       " {'1401.8203': ['gr-qc/9808028']},\n",
       " {'1401.8208': ['1202.4317',\n",
       "   '1404.4255',\n",
       "   'hep-th/0412030',\n",
       "   'astro-ph/0611816',\n",
       "   '1303.5076',\n",
       "   '1406.2417']},\n",
       " {'1401.8219': ['1307.6272', '1309.0386', '1312.2986']},\n",
       " {'1401.8230': []},\n",
       " {'1401.8154': ['math/0408008', '1308.1172']},\n",
       " {'1401.8156': []},\n",
       " {'1401.8181': []},\n",
       " {'1401.8182': []},\n",
       " {'1401.8201': ['1406.0349']},\n",
       " {'1401.8202': []},\n",
       " {'1401.8203': ['gr-qc/9808028']},\n",
       " {'1401.8208': ['1202.4317',\n",
       "   '1404.4255',\n",
       "   'hep-th/0412030',\n",
       "   'astro-ph/0611816',\n",
       "   '1303.5076',\n",
       "   '1406.2417']},\n",
       " {'1401.8219': ['1307.6272', '1309.0386', '1312.2986']},\n",
       " {'1401.8230': []},\n",
       " {'1401.8154': ['math/0408008', '1308.1172']},\n",
       " {'1401.8156': []},\n",
       " {'1401.8181': []},\n",
       " {'1401.8182': []},\n",
       " {'1401.8201': ['1406.0349']},\n",
       " {'1401.8202': []},\n",
       " {'1401.8203': ['gr-qc/9808028']},\n",
       " {'1401.8208': ['1202.4317',\n",
       "   '1404.4255',\n",
       "   'hep-th/0412030',\n",
       "   'astro-ph/0611816',\n",
       "   '1303.5076',\n",
       "   '1406.2417']},\n",
       " {'1401.8219': ['1307.6272', '1309.0386', '1312.2986']},\n",
       " {'1401.8230': []}]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[{\"1401.8154\": [\"math/0408008\", \"1308.1172\"]}, {\"1401.8156\": []}, {\"1401.8181\": []}, {\"1401.8182\": []}, {\"1401.8201\": [\"1406.0349\"]}, {\"1401.8202\": []}, {\"1401.8203\": [\"gr-qc/9808028\"]}, {\"1401.8208\": [\"1202.4317\", \"1404.4255\", \"hep-th/0412030\", \"astro-ph/0611816\", \"1303.5076\", \"1406.2417\"]}, {\"1401.8219\": [\"1307.6272\", \"1309.0386\", \"1312.2986\"]}, {\"1401.8230\": []}][{\"1401.8154\": [\"math/0408008\", \"1308.1172\"]}, {\"1401.8156\": []}, {\"1401.8181\": []}, {\"1401.8182\": []}, {\"1401.8201\": [\"1406.0349\"]}, {\"1401.8202\": []}, {\"1401.8203\": [\"gr-qc/9808028\"]}, {\"1401.8208\": [\"1202.4317\", \"1404.4255\", \"hep-th/0412030\", \"astro-ph/0611816\", \"1303.5076\", \"1406.2417\"]}, {\"1401.8219\": [\"1307.6272\", \"1309.0386\", \"1312.2986\"]}, {\"1401.8230\": []}]']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
