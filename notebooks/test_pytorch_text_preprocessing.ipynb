{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Steps:\n",
    "1. Get list of all text files. This will probably be in the millions\n",
    "2. Grab one text file and run it through python text_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(os.path.abspath(\"/home/arxiv/doc_intel_etl\"))\n",
    "import config\n",
    "import src.blob_data_transfer as blob_transfer\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "from transformers import DistilBertTokenizer\n",
    "'''\n",
    "When loading large data via CPU in the DataLoader and need to push it\n",
    "to GPU during training then we should set pin_memory to True as it will\n",
    "speed up the host device transfer\n",
    "source: https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723\n",
    "'''\n",
    "from torch.utils.data._utils.pin_memory import pin_memory\n",
    "import threading\n",
    "from torch._six import queue, container_abcs, string_classes\n",
    "from torch.utils.data._utils import MP_STATUS_CHECK_INTERVAL\n",
    "from itertools import cycle, islice, chain\n",
    "import random\n",
    "import torch\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'arxiv_training_data/pdfplumber/text/1991'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_list():\n",
    "    return blob_transfer.get_blob_list(prefix)\n",
    "def get_text_stream(file):\n",
    "    return blob_transfer.stream_blob(file).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextIterableDataset(IterableDataset):\n",
    "    '''\n",
    "    TO DO:\n",
    "    1. Create another file that has the tokenizer. This should probably\n",
    "    be the model.py file itself\n",
    "    '''\n",
    "    def __init__(self, data_list, seq_length=256, batch_size=2):\n",
    "        self.data_list = data_list\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        # set max sequence length to 30,0000 so that we can tokenize\n",
    "        # all text in a document and not have to run it more than\n",
    "        # once per document\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "            'distilbert-base-uncased',\n",
    "            model_max_length=30000)\n",
    "     \n",
    "    @property\n",
    "    def shuffle_data_list(self):\n",
    "        return random.sample(self.data_list, len(self.data_list))\n",
    "    \n",
    "    def get_text_stream(self, file):\n",
    "        return blob_transfer.stream_blob(file).decode()\n",
    "    \n",
    "    def tokenize_stream(self, stream):\n",
    "        return self.tokenizer.tokenize(stream.replace('\\n',''))\n",
    "    \n",
    "    def encode_seq(self, tokens):\n",
    "        '''\n",
    "        Take input tokens and convert it into form needed for model\n",
    "        '''\n",
    "        return tokenizer(tokens,\n",
    "                         max_length=self.seq_length,\n",
    "                         padding=True,\n",
    "                         truncation=True,\n",
    "                         is_pretokenized=True,\n",
    "                         return_tensors='pt',\n",
    "                        )\n",
    "    \n",
    "    def parse_file(self, file):\n",
    "        '''\n",
    "        1. stream the given text file from blob\n",
    "        2. run the text file through pre-processing then tokenizer\n",
    "        3. break it up into seq_length sizes and yield those\n",
    "        '''\n",
    "        stream = self.get_text_stream(file)\n",
    "        worker = torch.utils.data.get_worker_info()\n",
    "        worker_id = worker.id if worker is not None else -1\n",
    "        \n",
    "        out = []\n",
    "        token_stream = self.tokenize_stream(stream)\n",
    "        \n",
    "        for token_num, token in enumerate(token_stream):\n",
    "            if token is None:\n",
    "                break\n",
    "            out.append(token)\n",
    "            # seq_length - 2 so we can add cls and eos tokens\n",
    "            if len(out) == (self.seq_length - 2):\n",
    "                y = self.encode_seq(out)\n",
    "                out = []\n",
    "                yield y\n",
    "\n",
    "    def get_stream(self, data_list):\n",
    "        '''\n",
    "        This function will continue to pull tokens from the opened file\n",
    "        via parse files until it has filled a full batch size\n",
    "        '''\n",
    "        print(\"number of files to stream: \", len(data_list))\n",
    "        tmp = map(self.parse_file, iter(data_list))\n",
    "        out = chain.from_iterable(tmp)\n",
    "        return out\n",
    "\n",
    "    def __iter__(self):\n",
    "        chunk_size = len(self.data_list) // self.batch_size\n",
    "        return zip(\n",
    "                *[self.get_stream(self.data_list[i*chunk_size:\n",
    "                                                 (i+1)*chunk_size]) \n",
    "                  for i in range(self.batch_size)]\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def split_datasets(cls, data_list, seq_length, batch_size,\n",
    "                       max_workers):\n",
    "        \n",
    "        for n in range(max_workers, 0, -1):\n",
    "            if batch_size % n == 0:\n",
    "                num_workers = n\n",
    "                break\n",
    "                \n",
    "        split_size = batch_size // num_workers\n",
    "        num_files_per_worker = len(data_list) // num_workers\n",
    "        out = []\n",
    "        for i in range(num_workers):\n",
    "            start = i * num_files_per_worker\n",
    "            end = (i + 1) * num_files_per_worker\n",
    "            stream_files = data_list[start:end]\n",
    "            item = cls(stream_files, batch_size=split_size,\n",
    "                       seq_length=seq_length)\n",
    "            out.append(item)\n",
    "        return out\n",
    "\n",
    "            \n",
    "\n",
    "class MultiStreamDataLoader:\n",
    "    \n",
    "    def __init__(self, datasets, pin_memory=True):\n",
    "        self.datasets = datasets\n",
    "        self.pin_memory = pin_memory\n",
    "            \n",
    "    def get_stream_loaders(self):\n",
    "        dataloaders = [\n",
    "            DataLoader(dataset, num_workers=1, batch_size=None,\n",
    "                       pin_memory=True) \n",
    "            for dataset in self.datasets\n",
    "        ]\n",
    "        return zip(*dataloaders)\n",
    "            \n",
    "    def join_streams_thread(self, out_queue, device_id, done_event):\n",
    "        '''\n",
    "        additional thread putting data into a queue to be collected\n",
    "        from __iter__\n",
    "        '''\n",
    "        torch.set_num_threads(1)\n",
    "        torch.cuda.set_device(device_id)\n",
    "        \n",
    "        for idx, batch_parts in enumerate(self.get_stream_loaders()):\n",
    "            data = list(chain(*batch_parts))\n",
    "            \n",
    "            data = torch.cat([item[:, None] for item in data], dim=1)\n",
    "            if (\n",
    "                not done_event.is_self()\n",
    "                and not isinstance(data, ExceptionWrapper)\n",
    "            ):\n",
    "                data = pin_memory(data)\n",
    "            \n",
    "            out_queue.put(data, timeout=MP_STATUS_CHECK_INTERVAL)\n",
    "            \n",
    "        self._join_memory_thread_done_even.set()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        # thread for collation and memory pinning\n",
    "        if self.pin_memory:\n",
    "            self._join_memory_thread_done_event = threading.Event()\n",
    "            self._data_queue = queue.Queue()\n",
    "            self.join_memory_thread = threading.Thread(\n",
    "                target=self.join_streams_thread,\n",
    "                args=(\n",
    "                    self._data_queue,\n",
    "                    torch.cuda.current_device(),\n",
    "                    self._join_memory_thread_done_event,\n",
    "                ),\n",
    "            )\n",
    "            self.join_memory_thread.daemon = True\n",
    "            self.join_memory_thread.start()\n",
    "            \n",
    "            while not self._join_memory_thread_done_event.is_set():\n",
    "                batch = self._data_queue.get(timeout=100000)\n",
    "                batch = {'data':batch}\n",
    "                yield batch\n",
    "            self.join_memory_thread.join()\n",
    "        else:\n",
    "            # Single process\n",
    "            for batch_parts in self.get_stream_loaders():\n",
    "                batch = list(chain(*batch_parts))\n",
    "                yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = get_text_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = text_list[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files to stream: 7 \n",
      "number of files to stream:  7\n",
      "number of files to stream:  7\n",
      "number of files to stream:  7\n"
     ]
    }
   ],
   "source": [
    "datasets = TextIterableDataset.split_datasets(test_list, seq_length=500, batch_size=4, max_workers=4)\n",
    "loader = MultiStreamDataLoader(datasets, pin_memory=False)\n",
    "test = []\n",
    "for batch in islice(loader, 30):\n",
    "    test.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] # # al ( so z # # φ = λ # # φ for some λ ) such that ∆ # # φ = 0 and φ is a solution of β = 0, then w ( φ ) is also a solution of β = 0 for any function w. the simplest non # # con # # stan # # tf # # un # # ction obey # # ing the stated conditions is φ = ( a a + a a ) / 2. consequently ( since # # 1 2 3 4 # # in fact φ = a a on q ) 1 2 # # ∞ # # x # # n # # φ = − o ( a a ) ( 2. 49 ) n 1 2 # # n = 0 # # is a per # # tur # # bation with vanishing second order beta function, for any values of the # # con # # stan # # ts o. na # # cco # # rdi # # ng to our an # # sat # # z for the meaning of φ, the marginal operator ( 2. 49 ) corresponds to def # # or # # ming the quad # # ric q to a more general hyper # # sur # # face # # ∞ # # x # # na a = a a + o ( a a ). ( 2. 50 ) 3 4 1 2 n 1 2 # # n = 0 # # in comparing to matrix models, as we will see, the e # # igen # # val # # ue phase space will # # cor # # res # # pon # # d to the a − a plane, and the curve a a = 0 in the a − a plane will # # 1 2 3 4 1 2 # # cor # # res # # pon # # d to the fe # # rmi surface. the function a a will correspond to the standard # # 1 22 2 # # in # # verted harmonic os # # ci # # lla # # tor hamilton # # ian h = p − q, and the function on the # # right hand side of ( 2. 50 ) is the hamilton # # ian of a per # # tur # # bed matrix model. the problem can probably be analyzed more completely using a criterion of # # cha # # ud # # hur # # i and j. a. schwarz, who claim [ 12 ] that a per # # tu [SEP]'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test[18][1]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the text file and run it through pytorch custom iterabledataset and dataloader. A Dataset in our instance is an entire arxiv paper which can have multiple batches.\n",
    "We can't use the built in dataloader multiprocessing becuse it will then just send back multiple copies of the same dataset. We want to make the dataloader itself be parallized so probably just use python multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = text_list[0]\n",
    "output = parse_file(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'arxiv_training_data/pdfplumber/text/1991/hep-lat9107001.txt', 'container': 'arxiv', 'snapshot': None, 'blob_type': <BlobType.BlockBlob: 'BlockBlob'>, 'metadata': {}, 'encrypted_metadata': None, 'last_modified': datetime.datetime(2020, 7, 15, 13, 20, 11, tzinfo=datetime.timezone.utc), 'etag': '0x8D828C1CD6CC37C', 'size': 28866, 'content_range': None, 'append_blob_committed_block_count': None, 'page_blob_sequence_number': None, 'server_encrypted': True, 'copy': {'id': None, 'source': None, 'status': None, 'progress': None, 'completion_time': None, 'status_description': None, 'incremental_copy': None, 'destination_snapshot': None}, 'content_settings': {'content_type': 'application/octet-stream', 'content_encoding': None, 'content_language': None, 'content_md5': bytearray(b'W \\x16\\xbab\\xd5\\xdd\\xa3.\\xcc\\xc3\\xcb\\xe4\\xf0\\x05k'), 'content_disposition': None, 'cache_control': None}, 'lease': {'status': 'unlocked', 'state': 'available', 'duration': None}, 'blob_tier': 'Hot', 'blob_tier_change_time': None, 'blob_tier_inferred': True, 'deleted': None, 'deleted_time': None, 'remaining_retention_days': None, 'creation_time': datetime.datetime(2020, 7, 15, 13, 20, 11, tzinfo=datetime.timezone.utc), 'archive_status': None, 'encryption_key_sha256': None, 'encryption_scope': None, 'request_server_encrypted': None}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–',\n",
       " '91',\n",
       " '–',\n",
       " '31',\n",
       " '##fs',\n",
       " '##u',\n",
       " '-',\n",
       " 'sc',\n",
       " '##ri',\n",
       " '-',\n",
       " '91',\n",
       " '-',\n",
       " '94',\n",
       " '§',\n",
       " 'how',\n",
       " 'to',\n",
       " 'put',\n",
       " 'a',\n",
       " 'heavier',\n",
       " 'hi',\n",
       " '##ggs']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
